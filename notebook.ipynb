{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e9a495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de991b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and TreeNode class\n",
    "import numpy as np  # For numerical operations and arrays\n",
    "\n",
    "# Define the TreeNode class to represent each node in the decision tree\n",
    "class TreeNode:\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain):\n",
    "        # Initialize node attributes\n",
    "        self.data = data  # Dataset at this node\n",
    "        self.feature_idx = feature_idx  # Index of the feature used for splitting\n",
    "        self.feature_val = feature_val  # Value threshold for the split\n",
    "        self.prediction_probs = prediction_probs  # Class probabilities for leaf predictions\n",
    "        self.information_gain = information_gain  # Gain from this split (for debugging)\n",
    "        self.left = None  # Left child node (for values < feature_val)\n",
    "        self.right = None  # Right child node (for values >= feature_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5894af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: DecisionTree class definition\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0):\n",
    "        # Constructor to set hyperparameters\n",
    "        self.max_depth = max_depth  # Maximum depth to prevent overfitting\n",
    "        self.min_samples_leaf = min_samples_leaf  # Minimum samples per leaf to stop splitting\n",
    "        self.min_information_gain = min_information_gain  # Minimum gain required for a split\n",
    "        self.tree = None  # Root of the tree, built during training\n",
    "        self.labels_in_train = None  # Will be set during training (unique labels)\n",
    "\n",
    "    def entropy(self, class_probabilities):\n",
    "        # Calculate entropy for impurity measurement\n",
    "        # Entropy is low when classes are pure\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p > 0])\n",
    "\n",
    "    def find_label_probs(self, data):\n",
    "        # Compute class probabilities from labels in data\n",
    "        # Return a probability vector whose length equals number of classes seen in training\n",
    "        labels = data[:, -1]  # Last column is assumed to be labels\n",
    "        if self.labels_in_train is None:\n",
    "            # If labels not set yet (shouldn't happen during normal train flow), fall back to unique labels in this data\n",
    "            unique, counts = np.unique(labels, return_counts=True)\n",
    "            return counts / len(labels)\n",
    "        # Build probabilities for each class in self.labels_in_train order\n",
    "        probs = np.array([np.mean(labels == c) for c in self.labels_in_train], dtype=float)\n",
    "        return probs\n",
    "\n",
    "    def partition_entropy(self, partitions):\n",
    "        # Weighted entropy of child partitions after a split\n",
    "        total = sum(len(p) for p in partitions)\n",
    "        return sum(self.entropy(self.find_label_probs(np.array(p))) * len(p) / total for p in partitions if len(p) > 0)\n",
    "\n",
    "    def split(self, data, feature_idx, feature_val):\n",
    "        # Split data into two groups based on feature value\n",
    "        mask = data[:, feature_idx] < feature_val  # Boolean mask for left/right\n",
    "        g1 = data[mask]  # Left group\n",
    "        g2 = data[~mask]  # Right group\n",
    "        return g1, g2\n",
    "\n",
    "    def find_best_split(self, data):\n",
    "        # Find the best feature and value for splitting by minimizing partition entropy\n",
    "        min_part_entropy = 1e6  # Initialize to a large number\n",
    "        min_entropy_feature_idx = None\n",
    "        min_entropy_feature_val = None\n",
    "        g1_min, g2_min = None, None\n",
    "        for idx in range(data.shape[1] - 1):  # Loop over features (exclude label)\n",
    "            feature_val = np.median(data[:, idx])  # Use median as split value\n",
    "            g1, g2 = self.split(data, idx, feature_val)\n",
    "            # Pass the full partitions (rows) to partition_entropy so find_label_probs can extract labels\n",
    "            part_entropy = self.partition_entropy([g1, g2])\n",
    "            if part_entropy < min_part_entropy:\n",
    "                min_part_entropy = part_entropy\n",
    "                min_entropy_feature_idx = idx\n",
    "                min_entropy_feature_val = feature_val\n",
    "                g1_min, g2_min = g1, g2\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    def create_tree(self, data, current_depth):\n",
    "        # Recursively build the tree\n",
    "        if current_depth >= self.max_depth:  # Stop if max depth reached\n",
    "            return None\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self.find_best_split(data)\n",
    "        label_probabilities = self.find_label_probs(data)\n",
    "        node_entropy = self.entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy  # Gain = parent entropy - weighted child entropy\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "        # If one of the splits is empty (or too small), or gain too low, make this a leaf\n",
    "        if split_1_data is None or split_2_data is None:\n",
    "            return node\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:  # Stop if leaves too small\n",
    "            return node\n",
    "        if information_gain < self.min_information_gain:  # Stop if gain too low\n",
    "            return node\n",
    "        current_depth += 1  # Increment depth\n",
    "        node.left = self.create_tree(split_1_data, current_depth)  # Build left subtree\n",
    "        node.right = self.create_tree(split_2_data, current_depth)  # Build right subtree\n",
    "        return node\n",
    "\n",
    "    def train(self, X_train, Y_train):\n",
    "        # Train the model by building the tree\n",
    "        self.labels_in_train = np.unique(Y_train)  # Store unique labels for prediction (keeps consistent ordering)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)  # Combine features and labels\n",
    "        self.tree = self.create_tree(train_data, 0)  # Start building from root\n",
    "\n",
    "    def predict_one_sample(self, X):\n",
    "        # Predict probabilities for a single sample by traversing the tree\n",
    "        node = self.tree\n",
    "        # Traverse until a leaf node (leaf defined as both children None or no split feature)\n",
    "        while node is not None:\n",
    "            # If this node is a leaf (no split), return its stored probabilities\n",
    "            if node.feature_idx is None or (node.left is None and node.right is None):\n",
    "                return node.prediction_probs\n",
    "            # Otherwise continue traversal using the split\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                # If left child missing, return current node's probs\n",
    "                if node.left is None:\n",
    "                    return node.prediction_probs\n",
    "                node = node.left\n",
    "            else:\n",
    "                if node.right is None:\n",
    "                    return node.prediction_probs\n",
    "                node = node.right\n",
    "        # Fallback (should not typically reach here)\n",
    "        return np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "    def predict_proba(self, X_set):\n",
    "        # Get probabilities for a set of samples\n",
    "        # Using apply_along_axis will assemble a 2D array because each returned vector has consistent length\n",
    "        return np.apply_along_axis(self.predict_one_sample, 1, X_set)\n",
    "\n",
    "    def predict(self, X_set):\n",
    "        # Get class predictions by taking argmax of probabilities\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        return np.argmax(pred_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a6bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data loading and training example for Decision Tree\n",
    "import pandas as pd  # For data loading\n",
    "\n",
    "# Load Iris dataset\n",
    "df = pd.read_csv('Supervised Learning Datasets/Iris.csv')\n",
    "\n",
    "# Map species to numeric labels\n",
    "# Note: CSV uses 'Species' with values like 'Iris-setosa' so normalize before mapping\n",
    "label_map = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
    "species_clean = df['Species'].str.replace('Iris-', '', regex=False).str.lower()\n",
    "Y = species_clean.map(label_map).values\n",
    "\n",
    "# Select only numeric feature columns (drop Id and the Species column)\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values  # Features\n",
    "\n",
    "# Split data (80/20)\n",
    "idx = np.random.permutation(len(X))\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[idx[:split]], X[idx[split:]]\n",
    "Y_train, Y_test = Y[idx[:split]], Y[idx[split:]]\n",
    "\n",
    "# Train the model\n",
    "model = DecisionTree()  # Use default params or adjust e.g., max_depth=3\n",
    "model.train(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7281f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Prediction and evaluation for Decision Tree\n",
    "# Make predictions\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(preds == Y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")  # Typically ~0.95\n",
    "# Tip: Visualize tree or use sklearn for comparison if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "419da3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# RandomForest class (replacement for the placeholder)\n",
    "class RandomForest:\n",
    "\tdef __init__(self, n_estimators=5, max_depth=4, min_samples_leaf=1, min_information_gain=0.0,\n",
    "\t\t\t\t bootstrap_sample_size=None, random_state=None):\n",
    "\t\tself.n_estimators = n_estimators\n",
    "\t\tself.max_depth = max_depth\n",
    "\t\tself.min_samples_leaf = min_samples_leaf\n",
    "\t\tself.min_information_gain = min_information_gain\n",
    "\t\tself.bootstrap_sample_size = bootstrap_sample_size\n",
    "\t\tself.random_state = random_state\n",
    "\t\tself.rng = np.random.RandomState(random_state)\n",
    "\t\tself.trees = []\n",
    "\t\tself.labels_ = None  # global label set (from full training set)\n",
    "\n",
    "\tdef _create_bootstrap_indices(self, n_samples):\n",
    "\t\tbs = self.bootstrap_sample_size or n_samples\n",
    "\t\treturn self.rng.choice(n_samples, size=bs, replace=True)\n",
    "\n",
    "\tdef train(self, X_train, Y_train):\n",
    "\t\t# store global labels (consistent ordering used when averaging probabilities)\n",
    "\t\tself.labels_ = np.unique(Y_train)\n",
    "\t\tself.trees = []\n",
    "\t\tn = X_train.shape[0]\n",
    "\t\tfor i in range(self.n_estimators):\n",
    "\t\t\tidx = self._create_bootstrap_indices(n)\n",
    "\t\t\tXb, Yb = X_train[idx], Y_train[idx]\n",
    "\t\t\ttree = DecisionTree(max_depth=self.max_depth,\n",
    "\t\t\t\t\t\t\t\tmin_samples_leaf=self.min_samples_leaf,\n",
    "\t\t\t\t\t\t\t\tmin_information_gain=self.min_information_gain)\n",
    "\t\t\t# train tree on bootstrap sample\n",
    "\t\t\ttree.train(Xb, Yb)\n",
    "\t\t\t# keep trained tree\n",
    "\t\t\tself.trees.append(tree)\n",
    "\n",
    "\tdef _pad_probs_to_global(self, probs, tree_labels):\n",
    "\t\t# probs: (n_samples, k), tree_labels: array-like length k\n",
    "\t\tn_samples = probs.shape[0]\n",
    "\t\tn_classes = len(self.labels_)\n",
    "\t\tpadded = np.zeros((n_samples, n_classes), dtype=float)\n",
    "\t\t# map tree's label ordering into global ordering\n",
    "\t\tfor i, lbl in enumerate(tree_labels):\n",
    "\t\t\t# find index in global labels\n",
    "\t\t\ttry:\n",
    "\t\t\t\tj = int(np.where(self.labels_ == lbl)[0][0])\n",
    "\t\t\t\tpadded[:, j] = probs[:, i]\n",
    "\t\t\texcept IndexError:\n",
    "\t\t\t\t# label not in global labels (unlikely) -> skip\n",
    "\t\t\t\tcontinue\n",
    "\t\treturn padded\n",
    "\n",
    "\tdef predict_proba(self, X_set):\n",
    "\t\tif not self.trees:\n",
    "\t\t\traise RuntimeError(\"RandomForest not trained. Call .train() first.\")\n",
    "\t\t# collect padded probability arrays from each tree\n",
    "\t\tpadded_list = []\n",
    "\t\tfor tree in self.trees:\n",
    "\t\t\tprobs = tree.predict_proba(X_set)  # (n_samples, k)\n",
    "\t\t\ttree_labels = tree.labels_in_train if getattr(tree, \"labels_in_train\", None) is not None else np.unique(Y)\n",
    "\t\t\tpadded = self._pad_probs_to_global(probs, tree_labels)\n",
    "\t\t\tpadded_list.append(padded)\n",
    "\t\t# stack and average across trees -> (n_samples, n_classes)\n",
    "\t\tstacked = np.stack(padded_list, axis=0)\n",
    "\t\tavg = np.mean(stacked, axis=0)\n",
    "\t\treturn avg\n",
    "\n",
    "\tdef predict(self, X_set):\n",
    "\t\tprobs = self.predict_proba(X_set)\n",
    "\t\treturn np.argmax(probs, axis=1)\n",
    "model = RandomForest(n_estimators=10, max_depth=6, random_state=42)  # adjust params as needed\n",
    "model.train(X_train, Y_train)  # ensure forest is trained before predicting\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(preds == Y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")  # Expect high accuracy on Iris RandomForest class (replacement for the placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba7980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: LinearRegression class\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, n_iters=1000):\n",
    "        # Initialize hyperparameters\n",
    "        self.lr = lr  # Learning rate for gradient descent\n",
    "        self.n_iters = n_iters  # Number of iterations\n",
    "        self.weights = None  # Feature weights (slope)\n",
    "        self.bias = None  # Intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Train the model using gradient descent\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.random.rand(num_features)  # Random init for weights\n",
    "        self.bias = 0  # Init bias to 0\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias  # Forward pass\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))  # Gradient for weights\n",
    "            db = (1 / num_samples) * np.sum(y_pred - y)  # Gradient for bias\n",
    "            self.weights -= self.lr * dw  # Update weights\n",
    "            self.bias -= self.lr * db  # Update bias\n",
    "        return self  # Return self for chaining\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04b2541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X: 0 NaNs in y: 0\n",
      "Infinite in X: 0 Infinite in y: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data loading, normalization, and training\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv('Supervised Learning Datasets/headbrain.csv')\n",
    "X = df[['Head Size(cm^3)']].values\n",
    "y = df['Brain Weight(grams)'].values\n",
    "\n",
    "# Normalize feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "idx = np.random.permutation(len(X_scaled))\n",
    "split = int(0.8 * len(X_scaled))\n",
    "X_train, X_test = X_scaled[idx[:split]], X_scaled[idx[split:]]\n",
    "y_train, y_test = y[idx[:split]], y[idx[split:]]\n",
    "\n",
    "# Check for NaNs or infinite values in X and y before training\n",
    "print('NaNs in X:', np.isnan(X_train).sum(), 'NaNs in y:', np.isnan(y_train).sum())\n",
    "print('Infinite in X:', np.isinf(X_train).sum(), 'Infinite in y:', np.isinf(y_train).sum())\n",
    "if np.isnan(X_train).any() or np.isnan(y_train).any() or np.isinf(X_train).any() or np.isinf(y_train).any():\n",
    "    print('Warning: Data contains NaN or infinite values. Please clean your data.')\n",
    "else:\n",
    "    # Train with a lower learning rate\n",
    "    model = LinearRegression(lr=0.0001)  # Lower lr to help convergence\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a34217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1460190.75\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Prediction and evaluation\n",
    "# Predict\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# Mean Squared Error\n",
    "mse = np.mean((preds - y_test)**2)\n",
    "print(f\"MSE: {mse:.2f}\")  # Typically low for this linear relationship\n",
    "# Tip: Plot with matplotlib to visualize fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3d833",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 1: MyLogisticRegression class (safe version)\n",
    "import numpy as np\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=2000):\n",
    "        # Hyperparameters (lower learning rate for stability)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.w = None  # Weights\n",
    "        self.b = 0  # Bias\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Sigmoid activation for probabilities (clip for stability)\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost(self, H, Y, m):\n",
    "        # Binary cross-entropy loss (clip for log stability)\n",
    "        H = np.clip(H, 1e-8, 1 - 1e-8)\n",
    "        return -np.sum(Y * np.log(H) + (1 - Y) * np.log(1 - H)) / m\n",
    "\n",
    "    def cal_gradient(self, w, H, X, Y):\n",
    "        # Compute gradients\n",
    "        m = X.shape[1]\n",
    "        dw = np.dot(X, (H - Y).T) / m\n",
    "        db = np.sum(H - Y) / m\n",
    "        return {\"dw\": dw, \"db\": db}\n",
    "\n",
    "    def gradient_position(self, w, b, X, Y):\n",
    "        # Forward pass and cost/grads\n",
    "        m = X.shape[1]\n",
    "        H = self.sigmoid(np.dot(w.T, X) + b)\n",
    "        cost = self.cost(H, Y, m)\n",
    "        grads = self.cal_gradient(w, H, X, Y)\n",
    "        return grads, cost\n",
    "\n",
    "    def gradient_descent(self, w, b, X, Y, print_cost=False):\n",
    "        # Perform gradient descent\n",
    "        costs = []\n",
    "        for i in range(self.num_iterations):\n",
    "            grads, cost = self.gradient_position(w, b, X, Y)\n",
    "            dw = grads[\"dw\"]\n",
    "            db = grads[\"db\"]\n",
    "            w -= self.learning_rate * dw\n",
    "            b -= self.learning_rate * db\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        return {\"w\": w, \"b\": b}, grads, costs\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Input validation\n",
    "        if np.isnan(X).any() or np.isnan(Y).any() or np.isinf(X).any() or np.isinf(Y).any():\n",
    "            raise ValueError(\"Input data contains NaN or infinite values.\")\n",
    "        # Initialize and train\n",
    "        self.w = np.zeros((X.shape[0], 1))\n",
    "        self.b = 0\n",
    "        params, _, _ = self.gradient_descent(self.w, self.b, X, Y.T, print_cost=True)\n",
    "        self.w = params[\"w\"]\n",
    "        self.b = params[\"b\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict binary labels\n",
    "        H = self.sigmoid(np.dot(self.w.T, X) + self.b)\n",
    "        return (H >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data loading and preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Load bank dataset (note: separator is ';')\n",
    "df = pd.read_csv('Supervised Learning Datasets/bank.csv', sep=';')\n",
    "\n",
    "# Preprocess: One-hot encode categoricals, map y to 0/1\n",
    "df = pd.get_dummies(df, drop_first=True)  # Handle categoricals\n",
    "y = df['y_yes'].values  # Assuming 'y' becomes 'y_yes' after dummies\n",
    "X = df.drop('y_yes', axis=1).values\n",
    "\n",
    "# Split\n",
    "idx = np.random.permutation(len(X))\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[idx[:split]], X[idx[split:]]\n",
    "y_train, y_test = y[idx[:split]], y[idx[split:]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
